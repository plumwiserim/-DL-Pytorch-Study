{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06-09. 과적합(Overfitting)을 막는 방법들\n",
    "\n",
    "1. 데이터의 양을 늘리기\n",
    "- 데이터의 양이 적을 경우, 특정 패턴이나 노이즈까지 쉽게 암기하므로 과적합 현상이 발생할 확률이 늘어난다. \n",
    "- 따라서 데이터 양을 늘려서 일반적인 패턴을 학습하도록 만들어야 한다. \n",
    "- 데이터 증식/증강(Augmentation): 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리는 방법\n",
    "- 주로 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등의 방식으로 사용된다. \n",
    "\n",
    "\n",
    "\n",
    "2. 모델의 복잡도 줄이기\n",
    "- 인공 신경망의 복잡도는 은닉층이나 매개변수의 수로 결정된다. \n",
    "- 인공 신경망에서는 모델의 매개변수의 수를 모델의 수용력(capacity)라고도 한다. \n",
    "\n",
    "\n",
    "\n",
    "3. 가중치 규제(Regularization) 적용하기\n",
    "- L1 규제: 가중치 w들의 절대값 합계를 비용 함수에 추가한다. (= L1 norm)\n",
    "- L2 규제: 모든 가중치 w들의 제곱합을 비용 함수에 추가한다. (= L2 norm)\n",
    "\n",
    "    L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용하다. \\\n",
    "    L2 규제는 가중치들의 제곱을 최소화하므로 w의 값이 완전히 0이 되기보다는 0에 가까워지는 경향을 띈다. \\\n",
    "    L1 규제가 필요되는 이유가 없다면, 경험적으로 L2규제가 더 잘 동작하므로, L2규제를 더 권장한다. \\\n",
    "    인공 신경망에서는 L2 규제를 가중치 감쇠(weight decay)라고 한다. \\\n",
    "    파이토치에서는 옵티마이저의 weight_decay 매개변수를 설정함으로써 L2 규제를 적용한다. (default = 0)\n",
    "    \n",
    "```python \n",
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "4. 드롭아웃(Dropout)\n",
    "- 드롭아웃은 신경망의 일부를 사용하지 않는 방법이다. \n",
    "- 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적이다. \n",
    "- 학습 시에 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고,\n",
    "- 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것과 같은 효과를 낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
